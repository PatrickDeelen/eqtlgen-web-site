<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Cookbook for eQTLGen phase II analyses - eQTLGen Phase II</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="/eqtlgen-web-site/css/vendor/bootstrap/bootstrap.min.css">

    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="/eqtlgen-web-site/css/eqtlgen.css">

    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

</head>

<body>

<nav class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark fixed-top">
    <div class="container">
        <a class="navbar-brand" href="index.html">eQTLGen</a>
        <div class="btn-group align-items-center mr-auto">
            <button type="button" class="btn btn-link dropdown-toggle" data-toggle="dropdown" aria-expanded="false">
                /Phase II
                <span class="sr-only">Toggle Dropdown</span>
            </button>
            <div class="dropdown-menu">
                <a class="dropdown-item" href="/phase2">Phase II</a>
                <a class="dropdown-item" href="/">Phase I</a>
                <a class="dropdown-item" href="/sc">Single-cell</a>
            </div>
        </div>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
                data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
                aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="index.html">
                        Home
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="cohorts.html">
                        Consortium members
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="projects.html">
                        Projects
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="collaboration.html">
                        Collaborate
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="documents.html">
                        Documents
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="eQTLGen-p2-cookbook.html">
                        Cookbook
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="articles.html">
                        Publications
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="resources.html">
                        Recources
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://github.com/eQTLGen">
                        <span class="fa fa-github"></span>
                    </a>
                </li>
            </ul>
        </div>
    </div>
</nav>

<div class="container" style="margin-top: 75px">
    <p>This cookbook is for running genome-wide eQTL mapping in the consortium
settings, by using HASE (<a href="https://www.nature.com/articles/srep36076">Roschupkin et al,
2016</a>; <a href="https://github.com/roshchupkin/hase">original code
repo</a>). This method enables to run
of genome-wide high-dimensional association analyses in consortium
settings by limiting the size of the shared data while preserving
participant privacy.</p>

<p>In this cookbook we perform the needed steps for that: we do data QC,
remove related samples, do genotype phasing and imputation, convert
genotype data to the needed format, prepare covariates, convert
expression and covariate data into the needed format, encode the data,
calculate partial derivatives, prepare files for permuted data, and
organize <em>non-personal</em> encoded and data and partial derivatives for
sharing with the central site. We also include pipelines that will be
used for running meta-analyses on the central site. For eQTLGen phase II
we perform this analysis for <strong>whole blood</strong> and <strong>PBMC</strong> tissues.</p>

<h3 id="why-this-method">Why this method?</h3>

<p>Classical meta-analysis requires the performing full GWAS for every gene
(~20,000) by each cohort. Full summary statistics are large, meaning
that, per every cohort, <em>several terabytes</em> of data would be needed to
be shared with the central site. Sharing that much data is technically
very complicated.</p>

<p>In contrast, HASE method calculates matrices of aggregate statistics
(called partial derivatives) which are needed for running predefined
models and, additionally, encoded genotype and expression matrices. All
those matrices are in the efficient file format and relatively small
size, making it feasible to share those. Encoding means multiplying the
original data with the random matrix <em>F</em> (or its inverse, in case of
expression matrix), so that no person-level information is obtainable
from the genotype and expression matrices after encoding. However,
sharing partial derivatives and encoded matrices with the central site
enables to run of pre-defined association tests between every variant
and every gene for every dataset; and finally, running the meta-analysis
over datasets. An additional merit of HASE is that it is possible
adaptively drop covariates in the encoded association test (by slicing
the aggregate partial derivative matrices), therefore enabling to make
informed decisions on the most suitable set of covariates to include
into the final meta-analysis. Also, this method shifts most of the
computational burden into the central site, so individual cohorts do not
need to run ~20,000 GWAS’es in their respective HPCs.</p>

<p>You can see the specifics of the method here: <a href="https://www.nature.com/articles/srep36076">Roschupkin et al,
2016</a></p>

<h3 id="technical-support">Technical support</h3>

<p>In case of any issues when running this cookbook please contact Urmo
Võsa (urmo.vosa at gmail.com).</p>

<h3 id="prerequisites">Prerequisites</h3>

<h4 id="needed">Needed</h4>

<ol>
  <li>HPC with multiple cores and scheduling system.</li>
  <li>You need to have Bash &gt;=3.2 installed to your HPC.</li>
  <li>You need to have Java &gt;=8 installed to your HPC. This is needed
for running Nextflow pipeline management tool.</li>
</ol>

<h4 id="strongly-recommended">Strongly recommended</h4>

<ol>
  <li>Our pipelines expect that you have
<a href="https://sylabs.io/guides/3.5/user-guide/index.html#">Singularity</a>
configured and running in your HPC, for managing needed
tools/dependencies. This means that you don’t need to install many
programs to your HPC. If there is no way of using Singularity in
your HPC, please contact us and we will look into alternative ways
for dependency management.</li>
  <li>You might also need a few extra modules, so that Singularity would
run as expected. If this is the case, we recommend to check this
with your HPC documentation and/or tech support. E.g. in University
of Tartu HPC there is also module named <strong>squashFS</strong> needed, so that
Singularity would work correctly.</li>
  <li>You HPC should have access to the internet. This is needed for
downloading analysis pipelines from code repos, for automatic
download of the containers from container repos and for downloading
some reference files. If it is mandatory for you to work offline,
please contact us and we will adjust the pipeline for working
offline.</li>
</ol>

<h4 id="recommended">Recommended</h4>

<ol>
  <li><a href="https://git-scm.com/">git</a>. This is convenient for cloning analysis
pipelines from the code repos. However, you can also download those
manually.</li>
  <li>Current analysis pipelines and configurations are tailored for usage
with <a href="slurm.schedmd.com">Slurm</a> scheduler. However, it is
straightforward to add configurations for other commonly used
schedulers (e.g. TORQUE, SGE, etc). Therefore, if your HPC uses some
other scheduler, please contact us and we will make the
configuration file for that specific scheduler.</li>
</ol>

<h3 id="setup">Setup</h3>

<p>The analysis consists of four <a href="nextflow.io">Nextflow</a> pipelines which
are used to perform all needed tasks in the correct order. For each
pipeline, we provide a Slurm script template which should be
complemented/adjusted with your input paths and a few dataset-specific
settings. Those pipelines are:</p>

<ul>
  <li><a href="#1-data-qc">Data QC pipeline</a>: for doing automatic genotype and
gene expression quality control, constructing covariates, and
calculating some diagnostic summary statistics.</li>
  <li><a href="#2-genotype-imputation">Imputation pipeline</a>: for imputing the
quality-controlled genotype data to the common reference panel.</li>
  <li><a href="#3-genotype-conversion">Genotype conversion pipeline</a>: for
converting imputed genotype data into efficient <code class="language-plaintext highlighter-rouge">.hdf5</code> format.</li>
  <li><a href="#4-per-cohort-data-preparations">Per-cohort data preparations</a>: for
making per-cohort preparations, encoding data, calculating partial
derivatives, doing permutation, and organizing all needed files for
sharing with the central site.</li>
</ul>

<p>We will provide detailed instructions for each pipeline below, however
for the most optimal setup, we recommend you to first organize your
analysis folder as follows:</p>

<ul>
  <li>Make analysis root folder for this project, e.g. <code class="language-plaintext highlighter-rouge">eQTLGen_phase2</code>.</li>
  <li><em>Inside</em> this folder make another folder for Nextflow executable,
called <code class="language-plaintext highlighter-rouge">tools</code>. This path is specified in all the template scripts,
so that scripts find Nextflow executable.</li>
  <li><em>Inside</em> <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/tools</code> download and self-install Nextflow
executable, as specified
<a href="https://www.nextflow.io/docs/latest/getstarted.html#installation">here</a>.
You might need to load Java &gt;=8 before running self-install
(e.g. <code class="language-plaintext highlighter-rouge">module load [Java &gt;=8 module name in your HPC]</code>).</li>
  <li><em>Inside</em> <code class="language-plaintext highlighter-rouge">eQTLGen_phase2</code>, make additional separate folders for each
step of this analysis plan:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC</code></li>
      <li><code class="language-plaintext highlighter-rouge">eQTLGen_phase2/2_Imputation</code></li>
      <li><code class="language-plaintext highlighter-rouge">eQTLGen_phase2/3_ConvertVcf2Hdf5</code></li>
      <li><code class="language-plaintext highlighter-rouge">eQTLGen_phase2/4_PerCohortPreparations</code></li>
    </ul>
  </li>
  <li><em>Inside</em> each of those folders, you should clone/download the
corresponding pipeline. If git is available in your HPC, easiest is
to use command <code class="language-plaintext highlighter-rouge">git clone [repo of the pipeline]</code>. This yields a
pipeline folder e.g. for the first pipeline it will look like that:
<code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/dataqc</code>.</li>
  <li>We recommend to specify <code class="language-plaintext highlighter-rouge">output</code> (and, if needed, <code class="language-plaintext highlighter-rouge">input</code> etc.)
folder(s) for each step. E.g. <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/output</code>.</li>
  <li>Inside each pipeline folder is the script template using name format
<code class="language-plaintext highlighter-rouge">submit_*_template.sh</code>
e.g. <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/dataqc/submit_DataQc_pipeline_template.sh</code>.
You should adjust this according to your data (e.g. specify the path
to your input folder, add required HPC modules), and save it. For
better tracking, I usually rename it too:
<code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/dataqc/submit_DataQc_pipeline_EstBB_HT12v3.sh</code>.</li>
</ul>

<p>❗ To ease the process, we have pre-filled some paths in the templates,
assuming that you use the recommended folder structure. - You should
submit each pipeline from <em>inside</em> each pipeline folder. - The logic of
the cookbook is the following: the relevant output from the previous
step should be specified as the input for the next step in the
analysis. - E.g. QCd genotype files from the output of first DataQC
pipeline:
<code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/output/[Your Cohort Name]/outputfolder_gen/gen_data_QCd/</code>
are the input for the second pipeline (imputation).</p>

<h4 id="running-monitoring-and-debugging-nextflow-pipelines">Running, monitoring and debugging Nextflow pipelines</h4>

<p>Example commands here are based on Slurm scheduler, however can be
easily conveyed to other schedulers.</p>

<ul>
  <li>
    <p>In order to run Nextflow pipelines, you have to modify each template
script and load couple of modules which enable to run Nextflow and
Singularity containers. These very common modules should be
available in your HPC, however exact name and version might differ:
<strong>Java &gt;=8</strong> and <strong>Singularity</strong>. You might also need a few extra
modules, so that Singularity would run as expected. If this is the
case, we recommend to check this with your HPC documentation and/or
tech support. E.g. in University of Tartu HPC there is also module
named <strong>squashFS</strong> needed, so that Singularity would work correctly.</p>
  </li>
  <li>
    <p>When you submit the job
e.g. <code class="language-plaintext highlighter-rouge">sbatch submit_DataQc_pipeline_EstBB_HT12v3.sh</code>, this initiates
the pipeline, makes analysis environment (using singularity
container) and automatically submits all the pipeline steps in the
correct order and parallelized way. Separate <code class="language-plaintext highlighter-rouge">work</code> directory is
made to the pipeline folder and this contains all the interim files
of the pipeline.</p>
  </li>
  <li>
    <p>Monitoring:</p>

    <ul>
      <li>Monitor the <code class="language-plaintext highlighter-rouge">slurm-***.out</code> log file and check if all the steps
finish without error. Trick: command
<code class="language-plaintext highlighter-rouge">watch tail -n 20 slurm-***.out</code> helps you to interactively
monitor the status of the jobs.</li>
      <li>Use <code class="language-plaintext highlighter-rouge">squeue -u [YourUserName]</code> to see if individual tasks are in
the queue.</li>
    </ul>
  </li>
  <li>
    <p>If the pipeline crashes (e.g. due to wall time), you can just
resubmit the same script after the fixes. Nextflow does not rerun
completed steps and continues only from the steps which had not been
completed.</p>
  </li>
  <li>
    <p>When the work has finished, download and check the job report, for
potential errors or warnings. This file is automatically written to
your output folder <code class="language-plaintext highlighter-rouge">pipeline_info</code> subfolder. E.g.
<code class="language-plaintext highlighter-rouge">output/pipeline_info/DataQcReport.html</code>.</p>
  </li>
  <li>
    <p>When you need to do some debugging, then you can use the last
section of the aforementioned report to figure out in which
subfolder from <code class="language-plaintext highlighter-rouge">work</code> folder the actual step was run. You can then
navigate to this folder and investigate the following hidden files:</p>

    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">.command.sh</code>: script which was submitted</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">.command.log</code>: log file for seeing the analysis outputs/errors.</p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">.command.err</code>: file which lists the errors, if any.</p>

        <p>Hopefully those three files can give you some handle which might
have gone wrong. Contact us, if this is not informative.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>We have specified reasonable computational resources which each
pipeline step uses. However, in some cases these might work
differently, depending on HPC or dataset size. Resources for each
pipeline step are specified in <code class="language-plaintext highlighter-rouge">config/base.conf</code>. In this file
there are resources (RAM, CPU, walltime) for each step of the
pipeline, as currently specified. You can try to increase those, if
the pipeline crashes due to insufficient resources.</p>
  </li>
  <li>
    <p>For some analyses, <code class="language-plaintext highlighter-rouge">work</code> directory can become quite large. So, when
the pipeline is successfully finished, and you have checked the
output, you can delete <code class="language-plaintext highlighter-rouge">work</code> directory, in order to save this
storage space of your HPC. But this means that the pipeline will
restart from scratch, if you ever need to rerun this pipeline.</p>
  </li>
</ul>

<h3 id="data">Data</h3>

<p>For this analysis plan you need the following information and data: -
Name of your cohort. - Unimputed genotype data for your eQTL samples. -
Gene expression matrix from your eQTL samples (for this project: whole
blood or PBMC). - Genotype-to-expression linking file, linking sample
IDs between those two datasets.</p>

<h4 id="name-of-your-cohort">Name of your cohort</h4>

<p>An informative name for your dataset is one of the required inputs for
the majority of the pipelines in this cookbook. This could be the
combination of cohort name and expression platform. If you contribute
with multiple datasets from e.g. several batches, you could specify it
by adding <code class="language-plaintext highlighter-rouge">_batch1</code>. If you contribute with multiple datasets from
several ancestries, then you could specify this by adding ancestry
e.g. <code class="language-plaintext highlighter-rouge">_EUR</code>.</p>

<p>Example 1: For EstBB cohort I have 2 datasets <code class="language-plaintext highlighter-rouge">EstBB_HT12v3</code> and
<code class="language-plaintext highlighter-rouge">EstBB_RNAseq</code>.</p>

<p>Example 2: If there would be two EstBB RNAseq datasets, I would use:
<code class="language-plaintext highlighter-rouge">EstBB_RNAseq_batch1</code> and <code class="language-plaintext highlighter-rouge">EstBB_RNAseq_batch2</code>.</p>

<h4 id="genotype-data">Genotype data</h4>

<ul>
  <li>As an input we expect the set of unimputed plink
<a href="https://www.cog-genomics.org/plink/1.9/formats#bed">.bed</a>/<a href="https://www.cog-genomics.org/plink/1.9/formats#bim">.bim</a>/<a href="https://www.cog-genomics.org/plink/1.9/formats#fam">.fam</a>
files where chromosome positions are in <strong>hg19</strong>.</li>
  <li>Ideally, the <code class="language-plaintext highlighter-rouge">.fam</code> file should contain information about the
reported sex of the sample. This information is used in the
automatic data quality control step. If this information is not
available, you can still use those files but this specific quality
control step will be skipped.</li>
  <li>In order to take advantage of optimal pre-phasing, the input
genotype data should have several thousands genotype samples. This
means that if you have e.g. 7,000 samples in the available genotype
data and only 500 of those have expression available (eQTL samples),
please specify all 7,000 as the input of the analysis, not only eQTL
samples. The larger number of input genotype samples will yield
better pre-phasing and subsequent imputation. eQTL samples will be
filtered after the imputation step. Our pipelines will automatically
use up to 5,000 genotype samples in data QC and imputation
pipelines. If only eQTL samples have genotype data available, only
eQTL samples will be used.</li>
</ul>

<p>:exclamation: Standard pre-imputation genotype and sample quality
control will be done by our pipeline, so no need for thorough QC.</p>

<h4 id="gene-expression-data">Gene expression data</h4>

<ul>
  <li>Tab-delimited expression matrix.</li>
  <li>Each row contains a gene expression value and each column contains a
sample ID.</li>
  <li>The first column has column name <code class="language-plaintext highlighter-rouge">-</code> and contains either ENSEMBL
gene IDs (RNA-seq), Illumina ArrayAddress IDs for probes or probe
names for corresponding Affymetrix array.</li>
  <li>For RNA-seq and Illumina arrays we expect that data is raw: not
normalized and/or transformed. Our pipeline takes care of those
steps.</li>
  <li>Can be gzipped.</li>
</ul>

<p>:exclamation: For <strong>RNA-seq data</strong> we expect that the raw RNA-seq data
has already been appropriately processed and QC’d prior to generating
the count matrix (e.g. proper settings for the read mapping, etc.).</p>

<p>:exclamation: For <strong>Affymetrix arrays</strong> we expect that <strong>all</strong> the
standard data preprocessing steps for this array type have already been
applied (e.g. RMA normalization). On top of those, our pipeline just
applies inverse normal transformation on each gene (forces the
distribution to be normal).</p>

<h4 id="genotype-to-expression-gte-linking-file">Genotype-to-expression (GTE) linking file</h4>

<ul>
  <li>Genotype-to-expression (GTE) linking file: tab-delimited file with
sample ID matches between genotype data (column 1) and expression
data (column 2). If those IDs are the same in both data layers, you
can just use file with same sample IDs in both columns.</li>
</ul>

<p>If you participated in eQTLGen phase 1 analyses, this file should
already be available.</p>

<h4 id="mixups">Mixups</h4>

<p>We assume that potential sample mixups have already been identified,
resolved or removed. See details from
<a href="https://github.com/molgenis/systemsgenetics/wiki/eQTL-mapping-analysis-cookbook-(eQTLGen)#step-4---mixupmapper">here</a>.</p>

<h3 id="analysis-instructions">Analysis instructions</h3>

<h4 id="1-data-qc">1. Data QC</h4>

<p>In this step, we prepare the genotype and gene expression data for the
next steps. Specifically, the pipeline does the following steps:</p>

<ul>
  <li>Genotypes
    <ul>
      <li>Standard SNP QC filtering (call-rate&gt;0.95, Hardy-Weinberg
P&gt;1e-6, MAF&gt;0.01).</li>
      <li>Individual-level missingness filter &lt;0.05.</li>
      <li>Comparison of the reported and genetic check, removal of
mismatched samples.</li>
      <li>Removal of samples with unclear genetic sex.</li>
      <li>Removal of samples with excess heterozygosity (+/-3SD from the
mean).</li>
      <li>Removal of related samples (3d degree relatives). From each pair
of related samples, one is kept in the data.</li>
      <li>Visualisation of samples in the genetic reference space,
instructions on how to remove or split the data in case of
multi-ancestry samples.</li>
      <li>Removal of in-sample genetic outliers.</li>
      <li>Calculates 10 first genetic principal components (PCs), used in
analyses as covariates to correct for population stratification.</li>
    </ul>
  </li>
  <li>Gene expression:
    <ul>
      <li>Aligns sample IDs between genotype samples and gene expression
samples.</li>
      <li>Filters in blood-expressed genes.</li>
      <li>Replaces array probe IDs with gene IDs.</li>
      <li>Iteratively checks for expression outliers and removes samples
which fail this check.</li>
      <li>Normalises the data according to the expression platform used
and applies additional inverse normal transformation.</li>
      <li>Calculates 100 first expression PCs, used in analyses as
covariates to correct for unknown variation.</li>
      <li>Calculates the expression summary statistics for each gene, used
to do QC and gene filtering in the meta-analysis.</li>
    </ul>
  </li>
  <li>Additional steps:
    <ul>
      <li>Removes samples whose genetic sex does not match with the
expression of sex-specific genes.</li>
      <li>Reorders the genotype samples into random order.</li>
      <li>Organises all the QCd data into the standard folder format.</li>
      <li>Provides commented html QC report which should be used to get an
overview of the quality of the data.</li>
    </ul>
  </li>
</ul>

<h5 id="input-files">Input files</h5>

<ul>
  <li>Unimputed genotype file in plink
<a href="https://www.cog-genomics.org/plink/1.9/input#bed">.bed/.bim/.fam</a>
format. Genome build has to be in hg19. It is advisable that the
.fam file also includes observed sex for all samples (format:
males=1, females=2), so that pipeline does extra check on that.
However, if this information is not available for all samples, the
pipeline just skips this check.</li>
</ul>

<p>:exclamation: Because pre-phasing of genotypes benefits from larger
sample sizes and many eQTL datasets have modest sample sizes
(N&lt;1,000), it is not advisable to prefilter the unimputed genotype
data to include only eQTL samples. Our pipeline will extract eQTL
samples itself (based on the genotype-to-expression file) and, <em>if</em>
additional samples are available, includes additional up to 5,000
genotype samples which will be kept until the pre-phasing step of the
analysis. If the genotype data is available for &lt;5,000 samples, the
pipeline uses those samples which are avilable.</p>

<ul>
  <li>Raw, unprocessed gene expression matrix. Tab-delimited file,
genes/probes in the rows, samples in the columns.
    <ul>
      <li>First column has header “-”.</li>
      <li>For Illumina arrays, probe ID has to be Illumina ArrayAddress.</li>
      <li>For RNA-seq, gene ID has to be stable ENSEMBL gene ID (ENSEMBL
v75).</li>
      <li>For Affymetrix arrays we expect that gene expression matrix has
already gone through the standard preprocessing and is in the
same format as was used in eQTLGen phase 1 analyses (incl. array
probe names).</li>
    </ul>
  </li>
  <li>Genotype-to-expression linking file (GTE). Tab-delimited file, no
header, 2 columns: sample ID in genotype data, corresponding sample
ID in gene expression data. You can use this file to select samples
which go into the analysis.</li>
</ul>

<h5 id="instructions">Instructions</h5>

<ol>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC</code>.</p>
  </li>
  <li>
    <p>Clone the <code class="language-plaintext highlighter-rouge">dataqc</code> pipeline
<code class="language-plaintext highlighter-rouge">git clone https://github.com/eQTLGen/DataQC.git</code>.</p>
  </li>
  <li>
    <p>Make folder <code class="language-plaintext highlighter-rouge">output</code>.</p>
  </li>
  <li>
    <p>Optional: make folders <code class="language-plaintext highlighter-rouge">input/genotypes/</code>, <code class="language-plaintext highlighter-rouge">input/expression</code>,
<code class="language-plaintext highlighter-rouge">input/gte</code>. Copy or symlink the unimputed genotype files, gene
expression matrix and genotype-to-expression file into respective
folders. You can also specify the original location for each file in
the script template.</p>
  </li>
  <li>
    <p>Go into pipeline folder <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/1_DataQC/dataqc</code> and adjust
the script template with needed modules, input paths and settings.
Aside from the paths to your data files, you must specify:</p>

    <ol>
      <li>Expression platform of your expression data (<code class="language-plaintext highlighter-rouge">--exp_platform</code>).
Options: Illumina arrays: <code class="language-plaintext highlighter-rouge">HT12v3</code>, <code class="language-plaintext highlighter-rouge">HT12v4</code> or <code class="language-plaintext highlighter-rouge">HuRef8</code>;
RNA-seq: <code class="language-plaintext highlighter-rouge">RNAseq</code>; Affymetrix arrays: <code class="language-plaintext highlighter-rouge">AffyU219</code> or
<code class="language-plaintext highlighter-rouge">AffyHumanExon</code>.</li>
      <li>Informative name for your dataset (<code class="language-plaintext highlighter-rouge">--cohort_name</code>): your unique
dataset name used by all pipelines.</li>
    </ol>

    <p>This is the template for Slurm scheduler
(<code class="language-plaintext highlighter-rouge">submit_DataQC_pipeline_template.sh</code>):</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --time=48:00:00
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=6G
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --job-name="DataQc"

# These are needed modules in UT HPC to get singularity and Nextflow running. Replace with appropriate ones for your HPC.
module load java-1.8.0_40
module load singularity/3.5.3
module load squashfs/4.4

# Define paths
# If you follow eQTLGen phase II cookbook, you can use some provided default paths

nextflow_path=../../tools # folder where Nextflow executable is, can be kept as is.

geno_path=[full path to your input genotype file without .bed extension]
exp_path=[full path to your gene expression matrix]
gte_path=[full path to your genotype-to-expression file]
exp_platform=[expression platform name: HT12v3/HT12v4/HuRef8/RNAseq/AffyU219/AffyHumanExon]
cohort_name=[name of the cohort]
output_path=../output # Output path, can be kept as is.

# Optional arguments for the command
# --GenOutThresh [numeric threshold]
# --GenSdThresh [numeric threshold]
# --ExpSdThresh [numeric threshold]
# --ContaminationArea [number between 0 and 90, default 30]

# Command:
NXF_VER=21.10.6 ${nextflow_path}/nextflow run DataQC.nf \
--bfile ${geno_path} \
--expfile ${exp_path} \
--gte ${gte_path} \
--exp_platform ${exp_platform} \
--cohort_name ${cohort_name} \
--outdir ${output_path}  \
-profile slurm,singularity \
-resume
</code></pre></div>    </div>
  </li>
  <li>
    <p>Submit the job.</p>
  </li>
  <li>
    <p>Go into the output folder, download and investigate the thorough
report file <code class="language-plaintext highlighter-rouge">Report_DataQc_[your cohort name].html</code>. You will find
diagnostic plots and instructions on how to proceed from this
report.</p>
  </li>
  <li>
    <p>According to the instructions in the QC report, it is likely that
you need to adjust some of the QC thresholds. In your slurm script,
you can add the some or all of the arguments <code class="language-plaintext highlighter-rouge">--GenOutThresh</code>,
<code class="language-plaintext highlighter-rouge">--GenSdThresh</code>, <code class="language-plaintext highlighter-rouge">--ExpSdThresh</code>, <code class="language-plaintext highlighter-rouge">--ContaminationArea</code>, and specify
the appropriate values for each of those.</p>
  </li>
  <li>
    <p>In some cases, you might also need to split the data into several
batches, according to the ancestry. Then you should run this and
following pipelines for each of the batches separately.</p>
  </li>
  <li>
    <p>Resubmit the job.</p>
  </li>
  <li>
    <p>Go into the output folder, download and investigate the report file
<code class="language-plaintext highlighter-rouge">Report_DataQc_[your cohort name].html</code>. Now QC plots should look as
expected.</p>
  </li>
  <li>
    <p>If some of the plots still indicate issues, adjust the arguments and
re-run the pipeline. You might need to do so a couple of times until
there are no more apparent issues.</p>
  </li>
  <li>
    <p>:tada: Done!</p>
  </li>
</ol>

<h5 id="output">Output</h5>

<p>Pipeline gives output into the specified directory. The important files
are following.</p>

<ol>
  <li>Files: <code class="language-plaintext highlighter-rouge">output/outputfolder_gen/gen_data_QCd/*_ToImputation.bed</code>,
<code class="language-plaintext highlighter-rouge">output/outputfolder_gen/gen_data_QCd/*_ToImputation.bim</code>,
<code class="language-plaintext highlighter-rouge">output/outputfolder_gen/gen_data_QCd/*_ToImputation.fam</code> are the
filtered and QCd genotype files which need to be the input for the
next, <a href="#2-genotype-imputation">imputation pipeline</a>. You need to
specify the path to the <em>base</em> name (no extension <code class="language-plaintext highlighter-rouge">.bed/.bim/.fam</code>)
as an input.</li>
  <li>The whole <code class="language-plaintext highlighter-rouge">output</code> folder should be specified as one of the inputs
for <a href="#4-per-cohort-data-preparations">per-cohort preparations
pipeline</a>. This pipeline
automatically uses the processed, QCd expression data and covariate
file to run data encoding and partial derivative calculation. It
then organises the encoded matrices for sharing with the central
site. It also extracts some QC files, plots and summaries for
sharing with the central site.</li>
</ol>

<h4 id="2-genotype-imputation">2. Genotype imputation</h4>

<p>In this step we impute the QC’d genotype data to the recently released
1000G 30X WGS reference panel.</p>

<p>Specifically, this pipeline performs the following steps:</p>

<ul>
  <li>Lifts the unimputed genotype files to hg38/GRCh38 coordinates.</li>
  <li>Aligns unimputed genotypes to reference panel.</li>
  <li>Converts unimputed genotype files to <code class="language-plaintext highlighter-rouge">.vcf</code> format.</li>
  <li>Fixes alleles to match with the reference panel.</li>
  <li>Does another round of genotype QC (Hardy-Weinberg P-value &lt; 1e-6,
missingness &gt; 0.05, and minor allele frequency &lt; 0.01).</li>
  <li>Calculates individual-level missingness.</li>
  <li>Does genotype pre-phasing (with Eagle v2.4.1).</li>
  <li>Does genotype imputation (with Minimac4).</li>
  <li>Filters imputed genotypes to MAF&gt;0.01.</li>
</ul>

<h5 id="input-files-1">Input files</h5>

<ul>
  <li>Unimputed and QC’d genotype files in plink <code class="language-plaintext highlighter-rouge">.bed/.bim/.fam</code>. These
files are in the output of previous <a href="#1-data-qc">data QC pipeline</a>.
These are in the folder <code class="language-plaintext highlighter-rouge">output/outputfolder_gen/gen_data_QCd</code>. You
need to specify the path to the base name (no extension
<code class="language-plaintext highlighter-rouge">.bed/.bim/.fam</code>) as an input.</li>
  <li>Folder with all the needed reference files for pre-phasing and
imputation. We provide it <a href="TODO!!!">here</a>.</li>
  <li>Genotype-to-expression file.</li>
</ul>

<h5 id="instructions-1">Instructions</h5>

<ol>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/2_Imputation</code>.</p>
  </li>
  <li>
    <p>Clone the <code class="language-plaintext highlighter-rouge">eQTLGenImpute</code> pipeline
<code class="language-plaintext highlighter-rouge">git clone https://github.com/eQTLGen/eQTLGenImpute.git</code>.</p>
  </li>
  <li>
    <p>Make folder <code class="language-plaintext highlighter-rouge">output</code>.</p>
  </li>
  <li>
    <p>Download the zipped reference
<code class="language-plaintext highlighter-rouge">wget https://www.dropbox.com/s/6g58ygjg9d2fvbi/eQTLGenReferenceFiles.tar.gz?dl=1</code>.</p>
  </li>
  <li>
    <p>Because this file is ~30GB and can corrupt during the download, also
download corresponding md5sum file
<code class="language-plaintext highlighter-rouge">wget https://www.dropbox.com/s/ekfciajzevn6o1l/eQTLGenReferenceFiles.tar.gz.md5?dl=1</code>.</p>
  </li>
  <li>
    <p>Check if download was sucessful
<code class="language-plaintext highlighter-rouge">md5sum --check eQTLGenReferenceFiles.tar.gz.md5</code>. You should see
this message in your terminal: <code class="language-plaintext highlighter-rouge">eQTLGenReferenceFiles.tar.gz:OK</code>.</p>
  </li>
  <li>
    <p>Unzip the reference file <code class="language-plaintext highlighter-rouge">tar -xfvz eQTLGenReferenceFiles.tar.gz</code>.
This yields the folder named <code class="language-plaintext highlighter-rouge">hg38</code> which contains all needed
reference files.</p>
  </li>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/2_Imputation/eQTLGenImpute</code> pipeline folder
and adjust the imputation script template with needed modules and
inputs.</p>

    <ol>
      <li>Genotype path: path to unimputed and QC’d genotype files in
plink <code class="language-plaintext highlighter-rouge">.bed/.bim/.fam</code> format, ending with <code class="language-plaintext highlighter-rouge">*_ToImputation</code>.
These files are in the output of previous <a href="#1-data-qc">data QC
pipeline</a>, files end with <code class="language-plaintext highlighter-rouge">*_ToImputation.*</code>. These
are in the folder <code class="language-plaintext highlighter-rouge">output/outputfolder_gen/gen_data_QCd</code>. You
should specify the full path to those files but <strong>without file
extension</strong>.</li>
      <li>Path to reference folder (pre-filled).</li>
      <li>Output path: to the folder <code class="language-plaintext highlighter-rouge">output</code> you made.</li>
      <li>Cohort name: this should be the same as in <code class="language-plaintext highlighter-rouge">dataqc</code>.</li>
      <li>Path to the folder with reference files (pre-filled).</li>
    </ol>

    <p>This is the template for Slurm scheduler
(<code class="language-plaintext highlighter-rouge">submit_imputation_pipeline_template.sh</code>):</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --time=72:00:00
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=6G
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --job-name="ImputeGenotypes"

# These are needed modules in UT HPC to get Singularity and Nextflow running.
# Replace with appropriate ones for your HPC.
module load java-1.8.0_40
module load singularity/3.5.3
module load squashfs/4.4

# If you follow the eQTLGen phase II cookbook and analysis folder structure,
# some of the following paths are pre-filled.
# https://github.com/eQTLGen/eQTLGen-phase-2-cookbook/wiki/eQTLGen-phase-II-cookbook

# Define paths and arguments
nextflow_path=../../tools # folder where Nextflow executable is.
reference_path=../hg38 # folder where you unpacked the reference files.

cohort_name=[name of your cohort]
qc_input_folder=../../1_DataQC/output # folder with QCd genotype and expression data, output of DataQC pipeline.
output_path=../output/ # Output path.

# Command
NXF_VER=21.10.6 ${nextflow_path}/nextflow run eQTLGenImpute.nf \
--qcdata ${qc_input_folder} \
--target_ref ${reference_path}/ref_genome_QC/Homo_sapiens.GRCh38.dna.primary_assembly.fa \
--ref_panel_hg38 ${reference_path}/ref_panel_QC/30x-GRCh38_NoSamplesSorted \
--eagle_genetic_map ${reference_path}/phasing/genetic_map/genetic_map_hg38_withX.txt.gz \
--eagle_phasing_reference ${reference_path}/phasing/phasing_reference/ \
--minimac_imputation_reference ${reference_path}/imputation/ \
--cohort_name ${cohort_name} \
--outdir ${output_path}  \
-profile slurm,singularity \
-resume
</code></pre></div>    </div>
  </li>
  <li>
    <p>Submit the job.</p>
  </li>
  <li>
    <p>Check the <code class="language-plaintext highlighter-rouge">output/pipeline_info/imputation_report.html</code>.</p>
  </li>
  <li>
    <p>:tada: Done!</p>
  </li>
</ol>

<h5 id="output-1">Output</h5>

<p>The pipeline gives output into the specified directory. The important
files are following.</p>

<ol>
  <li>Folder <code class="language-plaintext highlighter-rouge">output/postimute/</code> should be specified as the input for the
next step, <a href="#3-genotype-conversion">genotype conversion</a>. This
folder contains imputed <code class="language-plaintext highlighter-rouge">.vcf.gz</code> files, filtered by MAF&gt;0.01.</li>
  <li>Folder <code class="language-plaintext highlighter-rouge">output/preimpute/</code> contains the formatted, filtered, and QCd
genotype data before imputation (<code class="language-plaintext highlighter-rouge">.vcf.gz</code>). This is not directly
needed for this cookbook and might be used for other applications,
debugging the imputation, or just deleted.</li>
</ol>

<h4 id="3-genotype-conversion">3. Genotype conversion</h4>

<p>In this step, we convert the imputed genotype files from <code class="language-plaintext highlighter-rouge">.vcf.gz</code>
format into efficient <code class="language-plaintext highlighter-rouge">.hdf5</code> format. This file format is native to HASE
and is needed for running the following HASE steps (encoding, partial
derivative computation) in a more efficient way.</p>

<p>Specifically, this pipeline performs the following steps:</p>

<ul>
  <li>It chunks <code class="language-plaintext highlighter-rouge">.vcf.gz</code> files into chunks of 25,000 variants for faster
parallel processing.</li>
  <li>It converts the chunks into <code class="language-plaintext highlighter-rouge">.hdf5</code> format.</li>
  <li>It outputs a file with summary and quality metrics for each SNP
included in the analysis. This includes MAF, Hardy-Weinberg P-value,
Mach R2 (a measure of imputation quality), genotype counts, call
rate (always 1 as imputed data) and an indicator whether SNP was
typed of imputed.</li>
</ul>

<blockquote>
  <p>These quality metrics will be used for performing additional
per-variant QC in the central site, if needed.</p>
</blockquote>

<ul>
  <li>It renames and organises files into custom hdf5 genotype folder
format so that these are usable in the HASE framework.</li>
</ul>

<h5 id="input-files-2">Input files</h5>

<ul>
  <li>Folder containing imputed genotype files in the bgzipped <code class="language-plaintext highlighter-rouge">vcf.gz</code>
format.</li>
</ul>

<h5 id="instructions-2">Instructions</h5>

<ol>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/3_ConvertVcf2Hdf5</code>.</p>
  </li>
  <li>
    <p>Clone the <code class="language-plaintext highlighter-rouge">ConvertVcf2Hdf5</code> pipeline
<code class="language-plaintext highlighter-rouge">git clone https://github.com/eQTLGen/ConvertVcf2Hdf5.git</code>.</p>
  </li>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">3_ConvertVcf2Hdf5/ConvertVcf2Hdf5</code> and adjust the genotype
conversion script template with needed modules and inputs.</p>
  </li>
  <li>
    <p>Adjust the template script.</p>

    <p>This is the template for Slurm scheduler
(<code class="language-plaintext highlighter-rouge">submit_genotype_conversion_template.sh</code>):</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --time=24:00:00
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=5G
#SBATCH --job-name="ConvertVcf2Hdf5"

# These are needed modules in UT HPC to get Singularity and Nextflow running.
# Replace with appropriate ones for your HPC.
module load java-1.8.0_40
module load singularity/3.5.3
module load squashfs/4.4

# Define paths
nextflow_path=../../tools/

genopath=[Folder with input genotype files .vcf.gz format]
outputpath=../output/
cohort_name=[]

NXF_VER=21.10.6 ${nextflow_path}/nextflow run ConvertVcf2Hdf5.nf \
--vcf ${genopath} \
--outdir ${outputpath} \
--cohort_name ${cohort_name} \
-profile slurm,singularity \
-resume
</code></pre></div>    </div>
  </li>
  <li>
    <p>Submit the job.</p>
  </li>
  <li>
    <p>Check the <code class="language-plaintext highlighter-rouge">output/pipeline_info/ConvertGenotype_report.html</code>.</p>
  </li>
  <li>
    <p>:tada: Done!</p>
  </li>
</ol>

<h5 id="output-2">Output</h5>

<p>After successful completion of the pipeline, there should be <code class="language-plaintext highlighter-rouge">hdf5</code>
genotype file structure in your output folder, which, in addition to
<code class="language-plaintext highlighter-rouge">pipeline_info</code>, contains folders named <code class="language-plaintext highlighter-rouge">individuals</code>, <code class="language-plaintext highlighter-rouge">probes</code>,
<code class="language-plaintext highlighter-rouge">genotype</code> and <code class="language-plaintext highlighter-rouge">SNPQC</code>. For eQTLGen phase 2 analyses, this folder is one
out of two inputs of <a href="#4-per-cohort-data-preparations">per-cohort data
preparations</a> pipeline.</p>

<h4 id="4-per-cohort-data-preparations">4. Per-cohort data preparations</h4>

<p>This step prepares the data and performs all the steps which are needed
for running encoded HASE meta-analysis in the central site.</p>

<p>Specifically, this pipeline performs the following steps:</p>

<ul>
  <li>Uses 1000G 30x reference file to create variant mapper files (to
make the SNPs from different studies jointly analyzable in central
site).</li>
  <li>Encodes genotype and gene expression data, and deletes the random
matrix used for encoding. This means that no information for
individual study participant is obtainable from encoded data, even
for the original cohort analyst.</li>
  <li>Calculates partial derivatives, needed for running the eQTL mapping.</li>
  <li>Permutes the sample links on the unencoded data, encodes, and
calculates partial derivatives for the permuted data. This is needed
for obtaining in-sample LD estimates for downstream analyses in the
central site (useful for e.g. multiple testing corrections and
fine-mapping).</li>
  <li>Associates expression PCs with genotypes, writes out suggestive
associations (P&lt;1×10<sup>-5</sup>). This enables us to make an
informed decision which covariates to include into encoded HASE
model in the central site and control for the collider effects.</li>
  <li>Replaces original sample IDs in the encoded data with
“CohortName__index”.</li>
  <li>Collects several summary reports, QC reports and diagnostic plots
from the output of <a href="#1-data-qc">data QC pipeline</a>.</li>
  <li>Organises all the partial derivates, encoded matrices and QC reports
into the structured folder structure, ready for sharing.</li>
  <li>Calculates <code class="language-plaintext highlighter-rouge">md5sum</code> for all the shared files, so the integrity of
the upload can be checked.</li>
</ul>

<h5 id="input-files-3">Input files</h5>

<ul>
  <li>Folder with genotype files in the <code class="language-plaintext highlighter-rouge">.hdf5</code> format. This folder is the
output of <a href="#3-genotype-conversion">genotype conversion pipeline</a>.</li>
  <li>Full path to the output folder of <a href="#1-data-qc">data QC pipeline</a>.
Pipeline automatically takes preprocessed expression matrix,
covariate file and several QC metric files from this folder
structure.</li>
</ul>

<h5 id="instructions-3">Instructions</h5>

<ol>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">eQTLGen_phase2/4_PerCohortPreparations</code>.</p>
  </li>
  <li>
    <p>Make folder <code class="language-plaintext highlighter-rouge">output</code></p>
  </li>
  <li>
    <p>Clone the <code class="language-plaintext highlighter-rouge">PerCohortPreparations</code> pipeline
<code class="language-plaintext highlighter-rouge">git clone https://github.com/eQTLGen/PerCohortDataPreparatons.git</code>.</p>
  </li>
  <li>
    <p>Go into <code class="language-plaintext highlighter-rouge">4_PerCohortPreparations/PerCohortPreparations</code>.</p>
  </li>
  <li>
    <p>Put genotype reference file into
<code class="language-plaintext highlighter-rouge">4_PerCohortPreparations/PerCohortPreparations/bin/hase/data/</code>
folder. This was already downloaded together with imputation
references in <a href="#2-genotype-imputation">Imputation step</a>. Therefore
you can copy it to the correct location like that:
<code class="language-plaintext highlighter-rouge">cp ../../2_Imputation/hg38/hase_reference/* bin/hase/data/.</code>.</p>
  </li>
  <li>
    <p>Adjust the template script.</p>

    <p>This is the template script for Slurm scheduler
(<code class="language-plaintext highlighter-rouge">submit_per_cohort_preparations_template.sh</code>):</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --time=24:00:00
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=5G
#SBATCH --job-name="RunDataPreparations"

# These are needed modules in UT HPC to get Singularity and Nextflow running.
# Replace with appropriate ones for your HPC.
module load java-1.8.0_40
module load singularity/3.5.3
module load squashfs/4.4

# If you follow the eQTLGen phase II cookbook and analysis folder structure,
# some of the following paths are pre-filled.
# https://github.com/eQTLGen/eQTLGen-phase-2-cookbook/wiki/eQTLGen-phase-II-cookbook
nextflow_path=../../tools

genotypes_hdf5=../../3_ConvertVcf2Hdf5/output # Folder with genotype files in .hdf5 format
qc_data_folder=../../1_DataQC/output # Folder containing QCd data, inc. expression and covariates
output_path=../output

NXF_VER=21.10.6 ${nextflow_path}/nextflow run PerCohortDataPreparations.nf \
--hdf5 ${genotypes_hdf5} \
--qcdata ${qc_data_folder} \
--outdir ${output_path} \
-profile slurm,singularity \
-resume
</code></pre></div>    </div>
  </li>
  <li>
    <p>Submit the job.</p>
  </li>
  <li>
    <p>Check the
<code class="language-plaintext highlighter-rouge">output/pipeline_info/PerCohortDataPreparations_report.html</code>.</p>
  </li>
  <li>
    <p>🎉 Done!</p>
  </li>
</ol>

<h5 id="output-3">Output</h5>

<ul>
  <li>In the <code class="language-plaintext highlighter-rouge">output</code> folder there is subfolder called
<code class="language-plaintext highlighter-rouge">[YourCohortName]_IntermediateFilesEncoded_to_upload</code>. This folder
contains all the non-personal files, logs, reports and should be
shared with the central site.</li>
  <li>In the <code class="language-plaintext highlighter-rouge">output</code> folder there is also file
<code class="language-plaintext highlighter-rouge">[YourCohortName]_IntermediateFilesEncoded_to_upload.md5</code>. This
should also be shared with the central site, in order to check the
integrity of uploaded files.</li>
</ul>

<h4 id="5-share-the-data-with-central-location">5. Share the data with central location</h4>

<p>The output folder from <a href="#4-per-cohort-data-preparations">per-cohort preparations
pipeline</a> named
<code class="language-plaintext highlighter-rouge">[YourCohortName]_IntermediateFilesEncoded_to_upload</code> and its
accompanying <code class="language-plaintext highlighter-rouge">.md5</code> file should be uploaded into our sftp server:
<strong>TBA</strong>.</p>

<p>The instructions for getting the SFTP account: <strong>TBA</strong>.</p>

<p>:exclamation: You might want to inform your HPC team about this project
and the planned upload. Although the sheer file size is much smaller
that would be for the classical meta-analysis, it is still ~50GB in case
of 500 samples and RNA-seq. For the few largest datasets, the upload
might be up to 500GB and might raise some red flags when monitoring the
data traffic ;).</p>

<p>:tada: Thank you, you have shared the data necessary for global eQTL
mapping! We will keep you updated and will re-contact if any additional
information is needed.</p>

<h3 id="central-analyses">Central analyses</h3>

<p>Following pipelines are for performing global <em>trans</em>-eQTL
<strong>meta</strong>-analysis in the central location and are here outlined FYI. If
you are cohort analyst, no further action is needed. However, feel free
to use this code and instructions to conduct further analyses in your
respective cohort.</p>

<p><a href="https://gitlab.com/eqtlgen-group/metaanalysis">Pipeline for running encoded
meta-analysis</a> - this is
for running encoded eQTL meta-analysis on one or several datasets.</p>

<p><a href="https://gitlab.com/eqtlgen-group/ExtractMetaAnalysisResults">Pipeline for extracting subsets of data from full summary
statistics</a> -
this is for extracting data subsets for subsequent interpretation.</p>

<p>:exclamation: TODO: add documentation for those repos.</p>

<h3 id="runtime-estimates">Runtime estimates</h3>

<p>Here are some runtime benchmarks for each of the steps in this cookbook:
these might help you to get some hint how much time running the
pipelines might take.</p>

<p>All benchmarks are for RNA-seq dataset with following features: - Final
eQTL sample size of 477 samples. - RNA-seq: up to 19,942 genes in
processed expression matrix. - GSA genotyping array: 700,078 variants in
unimputd data.</p>

<h4 id="data-qc">Data QC</h4>

<ul>
  <li>Initial number of samples in unimputed <code class="language-plaintext highlighter-rouge">.bed</code> genotype file: 1,052</li>
  <li>Initial number of SNPs in unimputed <code class="language-plaintext highlighter-rouge">.bed</code> genotype file: 700,078</li>
  <li>Initial number of samples in gene expression data: 1,074</li>
  <li>Initial number of gene in gene expression data: 48803</li>
  <li>Infrastructure: University HPC with ~150 compute nodes</li>
  <li>Scheduler: Slurm</li>
  <li>Dependency management: Singularity</li>
  <li>Time to run the pipeline: 25m</li>
  <li>CPU hours: 0.7h</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">output</code> directory: 344MB</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">work</code> subdirectory: 1.8GB</li>
</ul>

<p>Note: benchmarks to initial run with default settings, you probably need
to re-run at least once after adjusting the settings.</p>

<h4 id="imputation">Imputation</h4>

<ul>
  <li>Initial number of samples in QCd and unimputed <code class="language-plaintext highlighter-rouge">.bed</code> genotype file:</li>
  <li>Initial number of SNPs in QCd and unimpuated <code class="language-plaintext highlighter-rouge">.bed</code> genotype file:</li>
  <li>Infrastructure: University HPC with ~150 compute nodes</li>
  <li>Scheduler: Slurm</li>
  <li>Dependency management: Singularity</li>
  <li>Time to run the pipeline: ~4.5h</li>
  <li>CPU hours: ~80h</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">output</code> directory: ~500GB</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">work</code> subdirectory: ~500GB</li>
</ul>

<h4 id="genotype-conversion">Genotype conversion</h4>

<ul>
  <li>Number of samples in imputed <code class="language-plaintext highlighter-rouge">.vcf.gz</code> genotype data: ~4,000</li>
  <li>Number of variants in imputed <code class="language-plaintext highlighter-rouge">.vcf.gz</code> genotype data: ~4,000</li>
  <li>Infrastructure: University HPC with ~150 compute nodes</li>
  <li>Dependency management: Singularity</li>
  <li>Time to run the pipeline (without wall times): ~4.5h</li>
  <li>CPU hours: ~80h</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">output</code> directory: ~500GB</li>
  <li>Final size of <code class="language-plaintext highlighter-rouge">work</code> subdirectory: ~500GB</li>
</ul>

<h4 id="per-cohort-data-preparation">Per-cohort data preparation</h4>

<ul>
  <li>Infrastructure: University HPC with ~150 compute nodes</li>
  <li>Scheduler: Slurm</li>
  <li>Dependency management: Singularity</li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>This cookbook utilizes HASE (<a href="https://github.com/roshchupkin/hase">https://github.com/roshchupkin/hase</a>) and
some of its helper scripts originally developed by:</p>

<p>Gennady V. Roscupkin (Department of Epidemiology, Radiology and Medical
Informatics, Erasmus MC, Rotterdam, Netherlands)</p>

<p>Hieab H. Adams (Department of Epidemiology, Erasmus MC, Rotterdam,
Netherlands).</p>

<h3 id="changes-from-the-original-hase-repo">Changes from the original HASE repo</h3>

<p>Robert Warmerdam (Department of Genetics, University Medical Center
Groningen, University of Groningen, Groningen, Netherlands) modified the
original HASE and fixed some bugs. He also added <code class="language-plaintext highlighter-rouge">classic-meta</code> option
to HASE analyses which enables to perform (encoded) inverse-variance
weighted meta-analysis and has started implementing methods for encoded
interaction analysis.</p>

<p>Urmo Võsa (Institute of Genomics, University of Tartu, Tartu, Estonia)
incorporated it into Nextflow pipeline and applied some minor
customization to the parts of the code.</p>

<p><strong>Changes:</strong></p>

<ul>
  <li>Fixed bug causing an exception when more than 1000 individuals were
used.</li>
  <li>Resolved bug causing the <code class="language-plaintext highlighter-rouge">--intercept</code> option having no effect.</li>
  <li>Made version numbers of pip packages explicit.</li>
  <li>Added commentary to code in places.</li>
  <li>Lines 355-357 of hase.py were commented out because this caused
pipeline to crash when &gt;1 datasets were added.</li>
  <li>Line 355 of /hdgwas/data.py were changed
<code class="language-plaintext highlighter-rouge">self.chunk_size=10000 --&gt; self.chunk_size=20000</code>.</li>
  <li>For eQTLGen pipelines: removed folders with unit tests and test
data, in order to keep the tool lightweight.</li>
</ul>

<h3 id="citation">Citation</h3>

<p>Original method paper for HASE framework:</p>

<p><a href="https://www.nature.com/articles/srep36076">Roshchupkin, G. V. et al. HASE: Framework for efficient
high-dimensional association analyses. Sci. Rep. 6, 36076; doi:
10.1038/srep36076 (2016)</a></p>

<h3 id="contacts">Contacts</h3>

<p>For this Nextflow pipeline: urmo.vosa at gmail.com</p>

<p>For the method of HASE, find contacts from here:
<a href="https://github.com/roshchupkin/hase">https://github.com/roshchupkin/hase</a></p>

</div>
<footer class="pt-5">
    <hr>
    <div class="container text-center">
        <img src="figs/genoomika.png" height="100px">
        <img src="figs/rug.jpg" height="100px">
        <p class="m-0 text-center text-white">Copyright &copy; eQTLGen 2022</p>
    </div>
    <!-- /.container -->
</footer>

<!-- Bootstrap core JavaScript -->
<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
        integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
        crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"
        integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1"
        crossorigin="anonymous"></script>

</body>

</html>
